{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "13B6pqBrTB8b",
        "outputId": "8821e700-3047-4210-9e5a-a0e2a34de796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "üöÄ Amazon Reviews Analysis with spaCy NER and Rule-based Sentiment\n",
            "======================================================================\n",
            "üìÅ Upload your .bz2 files when prompted\n",
            "Please upload your .bz2 files (train.ft.txt.bz2 and test.ft.txt.bz2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26d379aa-fc21-42e7-a224-7d99912e3801\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26d379aa-fc21-42e7-a224-7d99912e3801\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.ft.txt.bz2 to test.ft.txt.bz2\n",
            "Saving train.ft.txt.bz2 to train.ft.txt.bz2\n",
            "\n",
            "Processing test.ft.txt.bz2...\n",
            "Loaded 500 reviews from test.ft.txt.bz2\n",
            "\n",
            "Processing train.ft.txt.bz2...\n",
            "Loaded 500 reviews from train.ft.txt.bz2\n",
            "\n",
            "üîç Analyzing test.ft.txt.bz2...\n",
            "üìä Processing 500 reviews\n",
            "\n",
            "üìà ANALYSIS RESULTS for test.ft.txt.bz2\n",
            "==================================================\n",
            "================================================================================\n",
            "AMAZON REVIEWS ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "Sample Analysis Results (showing first 5 reviews):\n",
            "------------------------------------------------------------\n",
            "\n",
            "Review 1:\n",
            "Text: Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evapor...\n",
            "Predicted Sentiment: positive (score: 3)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Pat (PERSON: People, including fictional)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "  - GREAT (ORG: Companies, agencies, institutions, etc.)\n",
            "  - YEARS (DATE: Absolute or relative dates or periods)\n",
            "  - One (CARDINAL: Numerals that do not fall under another type)\n",
            "  - EVERYBODY (ORG: Companies, agencies, institutions, etc.)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "Brands detected: GREAT, EVERYBODY\n",
            "----------------------------------------\n",
            "\n",
            "Review 2:\n",
            "Text: One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger...\n",
            "Predicted Sentiment: positive (score: 2)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - One (CARDINAL: Numerals that do not fall under another type)\n",
            "  - Chrono Trigger (PERSON: People, including fictional)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "----------------------------------------\n",
            "\n",
            "Review 3:\n",
            "Text: Batteries died within a year ...: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge....\n",
            "Predicted Sentiment: neutral (score: 0)\n",
            "Actual Label: negative\n",
            "Named Entities:\n",
            "  - a year (DATE: Absolute or relative dates or periods)\n",
            "  - 2003 (DATE: Absolute or relative dates or periods)\n",
            "  - about a year (DATE: Absolute or relative dates or periods)\n",
            "----------------------------------------\n",
            "\n",
            "Review 4:\n",
            "Text: works fine, but Maha Energy is better: Check out Maha Energy's website. Their Powerex MH-C204F charger works in 100 minutes for rapid charge, with option for slower charge (better for batteries). And ...\n",
            "Predicted Sentiment: neutral (score: 0)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Maha Energy (PERSON: People, including fictional)\n",
            "  - Their Powerex MH-C204F (ORG: Companies, agencies, institutions, etc.)\n",
            "  - 100 minutes (TIME: Times smaller than a day)\n",
            "  - 2200 (CARDINAL: Numerals that do not fall under another type)\n",
            "Brands detected: Their Powerex MH-C204F\n",
            "----------------------------------------\n",
            "\n",
            "Review 5:\n",
            "Text: Great for the non-audiophile: Reviewed quite a bit of the combo players and was hesitant due to unfavorable reviews and size of machines. I am weaning off my VHS collection, but don't want to replace ...\n",
            "Predicted Sentiment: positive (score: 2)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - VHS (ORG: Companies, agencies, institutions, etc.)\n",
            "Brands detected: VHS\n",
            "----------------------------------------\n",
            "\n",
            "SUMMARY STATISTICS (Total reviews analyzed: 500)\n",
            "==================================================\n",
            "\n",
            "Sentiment Distribution:\n",
            "  Positive: 262 (52.4%)\n",
            "  Neutral: 151 (30.2%)\n",
            "  Negative: 87 (17.4%)\n",
            "\n",
            "Top 10 Named Entities:\n",
            "  first (ORDINAL): 56\n",
            "  one (CARDINAL): 48\n",
            "  two (CARDINAL): 32\n",
            "  Sony (ORG): 22\n",
            "  Amazon (ORG): 18\n",
            "  Christmas (DATE): 17\n",
            "  5 (CARDINAL): 13\n",
            "  2 (CARDINAL): 13\n",
            "  One (CARDINAL): 12\n",
            "  Apple (ORG): 12\n",
            "\n",
            "Top Brands Detected:\n",
            "  Sony: 22\n",
            "  Amazon: 18\n",
            "  Apple: 12\n",
            "  Foundation: 7\n",
            "  eMarker: 6\n",
            "\n",
            "Top Products Detected:\n",
            "  Classic: 2\n",
            "  Leslie: 1\n",
            "  BamPow: 1\n",
            "  W80: 1\n",
            "  Transformers Fan: 1\n",
            "\n",
            "üìã Creating summary DataFrame for test.ft.txt.bz2...\n",
            "   review_id sentiment  sentiment_score  actual_label  num_entities  \\\n",
            "0          0  positive                3             2             7   \n",
            "1          1  positive                2             2             5   \n",
            "2          2   neutral                0             1             3   \n",
            "3          3   neutral                0             2             4   \n",
            "4          4  positive                2             2             1   \n",
            "\n",
            "   num_brands  num_products  review_length  \n",
            "0           2             0            203  \n",
            "1           0             0            203  \n",
            "2           0             0            203  \n",
            "3           1             0            203  \n",
            "4           1             0            203  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'export_results_to_csv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3145660885>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;31m# Option 1: Full analysis (recommended)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;31m# Option 2: Quick start for testing (uncomment to use instead)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3145660885>\u001b[0m in \u001b[0;36mmain_colab\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Export option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mexport_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_analysis_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mexport_results_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüíæ Results saved as {export_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'export_results_to_csv' is not defined"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import bz2\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# Install required packages in Colab\n",
        "!pip install spacy pandas\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import required libraries\n",
        "import spacy\n",
        "import bz2\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def upload_and_load_data(max_samples=1000):\n",
        "    \"\"\"\n",
        "    Upload and load Amazon review data from bz2 compressed file in Colab\n",
        "    Expected format: __label__1 or __label__2 followed by review text\n",
        "    \"\"\"\n",
        "    print(\"Please upload your .bz2 files (train.ft.txt.bz2 and test.ft.txt.bz2)\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"\\nProcessing {filename}...\")\n",
        "        reviews = []\n",
        "        labels = []\n",
        "\n",
        "        try:\n",
        "            # Read the uploaded file\n",
        "            file_content = uploaded[filename]\n",
        "\n",
        "            # Decompress and read\n",
        "            with bz2.open(io.BytesIO(file_content), 'rt', encoding='utf-8') as f:\n",
        "                for i, line in enumerate(f):\n",
        "                    if i >= max_samples:  # Limit for demo purposes\n",
        "                        break\n",
        "\n",
        "                    line = line.strip()\n",
        "                    if line.startswith('__label__'):\n",
        "                        # Extract label and review text\n",
        "                        parts = line.split(' ', 1)\n",
        "                        if len(parts) == 2:\n",
        "                            label = parts[0].replace('__label__', '')\n",
        "                            review_text = parts[1]\n",
        "\n",
        "                            labels.append(int(label))\n",
        "                            reviews.append(review_text)\n",
        "\n",
        "            datasets[filename] = (reviews, labels)\n",
        "            print(f\"Loaded {len(reviews)} reviews from {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "            datasets[filename] = ([], [])\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def load_amazon_data_colab(file_content, max_samples=1000):\n",
        "    \"\"\"\n",
        "    Load Amazon review data from uploaded file content\n",
        "    Expected format: __label__1 or __label__2 followed by review text\n",
        "    \"\"\"\n",
        "    reviews = []\n",
        "    labels = []\n",
        "\n",
        "    try:\n",
        "        # Decompress and read\n",
        "        with bz2.open(io.BytesIO(file_content), 'rt', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= max_samples:  # Limit for demo purposes\n",
        "                    break\n",
        "\n",
        "                line = line.strip()\n",
        "                if line.startswith('__label__'):\n",
        "                    # Extract label and review text\n",
        "                    parts = line.split(' ', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        label = parts[0].replace('__label__', '')\n",
        "                        review_text = parts[1]\n",
        "\n",
        "                        labels.append(int(label))\n",
        "                        reviews.append(review_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    return reviews, labels\n",
        "\n",
        "class RuleBasedSentimentAnalyzer:\n",
        "    \"\"\"Simple rule-based sentiment analyzer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Positive and negative word lists\n",
        "        self.positive_words = {\n",
        "            'excellent', 'amazing', 'great', 'good', 'fantastic', 'wonderful',\n",
        "            'perfect', 'love', 'awesome', 'brilliant', 'outstanding', 'superb',\n",
        "            'impressed', 'satisfied', 'recommend', 'best', 'quality', 'happy',\n",
        "            'pleased', 'smooth', 'fast', 'easy', 'comfortable', 'durable'\n",
        "        }\n",
        "\n",
        "        self.negative_words = {\n",
        "            'bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'disappointed',\n",
        "            'poor', 'cheap', 'broken', 'defective', 'useless', 'waste', 'annoying',\n",
        "            'frustrating', 'slow', 'difficult', 'uncomfortable', 'flimsy', 'terrible',\n",
        "            'pathetic', 'garbage', 'trash', 'regret', 'problem', 'issue', 'fail'\n",
        "        }\n",
        "\n",
        "        # Negation words that can flip sentiment\n",
        "        self.negation_words = {'not', 'no', 'never', 'none', 'nothing', 'nowhere',\n",
        "                              'neither', 'nobody', 'hardly', \"don't\", \"doesn't\",\n",
        "                              \"didn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\"}\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment of text using rule-based approach\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        words = text_lower.split()\n",
        "\n",
        "        positive_score = 0\n",
        "        negative_score = 0\n",
        "\n",
        "        # Check for negation context\n",
        "        negated = False\n",
        "        for i, word in enumerate(words):\n",
        "            # Reset negation after 3 words\n",
        "            if i > 0 and i % 3 == 0:\n",
        "                negated = False\n",
        "\n",
        "            if word in self.negation_words:\n",
        "                negated = True\n",
        "                continue\n",
        "\n",
        "            # Clean word (remove punctuation)\n",
        "            clean_word = re.sub(r'[^\\w]', '', word)\n",
        "\n",
        "            if clean_word in self.positive_words:\n",
        "                if negated:\n",
        "                    negative_score += 1\n",
        "                else:\n",
        "                    positive_score += 1\n",
        "            elif clean_word in self.negative_words:\n",
        "                if negated:\n",
        "                    positive_score += 1\n",
        "                else:\n",
        "                    negative_score += 1\n",
        "\n",
        "        # Determine overall sentiment\n",
        "        if positive_score > negative_score:\n",
        "            return 'positive', positive_score - negative_score\n",
        "        elif negative_score > positive_score:\n",
        "            return 'negative', negative_score - positive_score\n",
        "        else:\n",
        "            return 'neutral', 0\n",
        "\n",
        "def extract_entities_and_sentiment(reviews, labels=None):\n",
        "    \"\"\"Extract named entities and analyze sentiment from reviews\"\"\"\n",
        "    sentiment_analyzer = RuleBasedSentimentAnalyzer()\n",
        "\n",
        "    results = []\n",
        "    entity_counter = Counter()\n",
        "    brand_counter = Counter()\n",
        "    product_counter = Counter()\n",
        "\n",
        "    for i, review in enumerate(reviews):\n",
        "        # Process with spaCy\n",
        "        doc = nlp(review)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = []\n",
        "        brands = []\n",
        "        products = []\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entity_info = {\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'description': spacy.explain(ent.label_)\n",
        "            }\n",
        "            entities.append(entity_info)\n",
        "            entity_counter[f\"{ent.text} ({ent.label_})\"] += 1\n",
        "\n",
        "            # Categorize potential brands and products\n",
        "            if ent.label_ in ['ORG', 'PRODUCT']:\n",
        "                if ent.label_ == 'ORG':\n",
        "                    brands.append(ent.text)\n",
        "                    brand_counter[ent.text] += 1\n",
        "                elif ent.label_ == 'PRODUCT':\n",
        "                    products.append(ent.text)\n",
        "                    product_counter[ent.text] += 1\n",
        "\n",
        "        # Analyze sentiment\n",
        "        sentiment, score = sentiment_analyzer.analyze_sentiment(review)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'review_id': i,\n",
        "            'review_text': review[:200] + '...' if len(review) > 200 else review,\n",
        "            'actual_label': labels[i] if labels else None,\n",
        "            'entities': entities,\n",
        "            'brands': brands,\n",
        "            'products': products,\n",
        "            'sentiment': sentiment,\n",
        "            'sentiment_score': score\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results, entity_counter, brand_counter, product_counter\n",
        "\n",
        "def display_results(results, entity_counter, brand_counter, product_counter, num_samples=5):\n",
        "    \"\"\"Display analysis results\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"AMAZON REVIEWS ANALYSIS RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Sample results\n",
        "    print(f\"\\nSample Analysis Results (showing first {num_samples} reviews):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, result in enumerate(results[:num_samples]):\n",
        "        print(f\"\\nReview {result['review_id'] + 1}:\")\n",
        "        print(f\"Text: {result['review_text']}\")\n",
        "        print(f\"Predicted Sentiment: {result['sentiment']} (score: {result['sentiment_score']})\")\n",
        "        if result['actual_label']:\n",
        "            actual_sentiment = 'positive' if result['actual_label'] == 2 else 'negative'\n",
        "            print(f\"Actual Label: {actual_sentiment}\")\n",
        "\n",
        "        if result['entities']:\n",
        "            print(\"Named Entities:\")\n",
        "            for ent in result['entities']:\n",
        "                print(f\"  - {ent['text']} ({ent['label']}: {ent['description']})\")\n",
        "\n",
        "        if result['brands']:\n",
        "            print(f\"Brands detected: {', '.join(result['brands'])}\")\n",
        "\n",
        "        if result['products']:\n",
        "            print(f\"Products detected: {', '.join(result['products'])}\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\nSUMMARY STATISTICS (Total reviews analyzed: {len(results)})\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Sentiment distribution\n",
        "    sentiment_dist = Counter([r['sentiment'] for r in results])\n",
        "    print(\"\\nSentiment Distribution:\")\n",
        "    for sentiment, count in sentiment_dist.items():\n",
        "        percentage = (count / len(results)) * 100\n",
        "        print(f\"  {sentiment.capitalize()}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Top entities\n",
        "    print(f\"\\nTop 10 Named Entities:\")\n",
        "    for entity, count in entity_counter.most_common(10):\n",
        "        print(f\"  {entity}: {count}\")\n",
        "\n",
        "    # Top brands\n",
        "    if brand_counter:\n",
        "        print(f\"\\nTop Brands Detected:\")\n",
        "        for brand, count in brand_counter.most_common(5):\n",
        "            print(f\"  {brand}: {count}\")\n",
        "\n",
        "    # Top products\n",
        "    if product_counter:\n",
        "        print(f\"\\nTop Products Detected:\")\n",
        "        for product, count in product_counter.most_common(5):\n",
        "            print(f\"  {product}: {count}\")\n",
        "\n",
        "def main_colab():\n",
        "    \"\"\"Main function to run the analysis in Google Colab\"\"\"\n",
        "    print(\"üöÄ Amazon Reviews Analysis with spaCy NER and Rule-based Sentiment\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìÅ Upload your .bz2 files when prompted\")\n",
        "\n",
        "    # Upload and load data\n",
        "    datasets = upload_and_load_data(max_samples=500)  # Reduced for Colab performance\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for filename, (reviews, labels) in datasets.items():\n",
        "        if not reviews:\n",
        "            print(f\"‚ö†Ô∏è  No data loaded from {filename}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüîç Analyzing {filename}...\")\n",
        "        print(f\"üìä Processing {len(reviews)} reviews\")\n",
        "\n",
        "        # Extract entities and sentiment\n",
        "        results, entities, brands, products = extract_entities_and_sentiment(reviews, labels)\n",
        "\n",
        "        # Store results\n",
        "        all_results[filename] = {\n",
        "            'results': results,\n",
        "            'entities': entities,\n",
        "            'brands': brands,\n",
        "            'products': products\n",
        "        }\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nüìà ANALYSIS RESULTS for {filename}\")\n",
        "        print(\"=\"*50)\n",
        "        display_results(results, entities, brands, products)\n",
        "\n",
        "        # Create and display DataFrame\n",
        "        print(f\"\\nüìã Creating summary DataFrame for {filename}...\")\n",
        "        df = create_summary_dataframe(results)\n",
        "        print(df.head())\n",
        "\n",
        "        # Export option\n",
        "        export_filename = filename.replace('.bz2', '_analysis_results.csv')\n",
        "        export_results_to_csv(results, export_filename)\n",
        "\n",
        "        print(f\"\\nüíæ Results saved as {export_filename}\")\n",
        "        print(\"üì• You can download it from the Files panel in Colab\")\n",
        "\n",
        "    # Comparison if multiple datasets\n",
        "    if len(all_results) > 1:\n",
        "        print(f\"\\nüîç DATASET COMPARISON\")\n",
        "        print(\"=\"*50)\n",
        "        compare_datasets(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def create_summary_dataframe(results):\n",
        "    \"\"\"Create a pandas DataFrame summary of results\"\"\"\n",
        "    data = []\n",
        "    for result in results:\n",
        "        row = {\n",
        "            'review_id': result['review_id'],\n",
        "            'sentiment': result['sentiment'],\n",
        "            'sentiment_score': result['sentiment_score'],\n",
        "            'actual_label': result['actual_label'],\n",
        "            'num_entities': len(result['entities']),\n",
        "            'num_brands': len(result['brands']),\n",
        "            'num_products': len(result['products']),\n",
        "            'review_length': len(result['review_text'])\n",
        "        }\n",
        "        data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def compare_datasets(all_results):\n",
        "    \"\"\"Compare results across different datasets\"\"\"\n",
        "    for filename, data in all_results.items():\n",
        "        results = data['results']\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_reviews = len(results)\n",
        "        sentiment_dist = Counter([r['sentiment'] for r in results])\n",
        "        avg_entities = sum(len(r['entities']) for r in results) / total_reviews\n",
        "\n",
        "        print(f\"\\nüìä {filename}:\")\n",
        "        print(f\"   Total reviews: {total_reviews}\")\n",
        "        print(f\"   Positive: {sentiment_dist.get('positive', 0)} ({sentiment_dist.get('positive', 0)/total_reviews*100:.1f}%)\")\n",
        "        print(f\"   Negative: {sentiment_dist.get('negative', 0)} ({sentiment_dist.get('negative', 0)/total_reviews*100:.1f}%)\")\n",
        "        print(f\"   Neutral: {sentiment_dist.get('neutral', 0)} ({sentiment_dist.get('neutral', 0)/total_reviews*100:.1f}%)\")\n",
        "        print(f\"   Avg entities per review: {avg_entities:.1f}\")\n",
        "\n",
        "# Quick start function for Colab\n",
        "def quick_start():\n",
        "    \"\"\"Quick start function with smaller sample size for testing\"\"\"\n",
        "    print(\"üöÄ QUICK START - Amazon Reviews Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"This will process a smaller sample (100 reviews per file) for quick testing\")\n",
        "\n",
        "    datasets = upload_and_load_data(max_samples=100)\n",
        "\n",
        "    for filename, (reviews, labels) in datasets.items():\n",
        "        if reviews:\n",
        "            print(f\"\\nüîç Quick analysis of {filename} ({len(reviews)} reviews):\")\n",
        "            results, entities, brands, products = extract_entities_and_sentiment(reviews, labels)\n",
        "\n",
        "            # Show just summary stats\n",
        "            sentiment_dist = Counter([r['sentiment'] for r in results])\n",
        "            print(f\"   Sentiment: {dict(sentiment_dist)}\")\n",
        "            print(f\"   Top entities: {dict(entities.most_common(3))}\")\n",
        "            print(f\"   Brands found: {list(brands.keys())[:5]}\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For Google Colab - choose your preferred method:\n",
        "\n",
        "    # Option 1: Full analysis (recommended)\n",
        "    results = main_colab()\n",
        "\n",
        "    # Option 2: Quick start for testing (uncomment to use instead)\n",
        "    # results = quick_start()\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis complete! Check the results above and download CSV files from the Files panel.\")\n",
        "\n",
        "# Additional utility functions for further analysis\n",
        "\n",
        "def export_results_to_csv(results, filename):\n",
        "    \"\"\"Export results to CSV for further analysis\"\"\"\n",
        "    data = []\n",
        "    for result in results:\n",
        "        row = {\n",
        "            'review_id': result['review_id'],\n",
        "            'review_text': result['review_text'],\n",
        "            'sentiment': result['sentiment'],\n",
        "            'sentiment_score': result['sentiment_score'],\n",
        "            'actual_label': result['actual_label'],\n",
        "            'num_entities': len(result['entities']),\n",
        "            'entities': '; '.join([f\"{e['text']}({e['label']})\" for e in result['entities']]),\n",
        "            'brands': '; '.join(result['brands']),\n",
        "            'products': '; '.join(result['products'])\n",
        "        }\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Results exported to {filename}\")\n",
        "\n",
        "# Example usage for CSV export:\n",
        "# export_results_to_csv(train_results, 'train_analysis_results.csv')\n",
        "# export_results_to_csv(test_results, 'test_analysis_results.csv')"
      ]
    },
    {
      "source": [
        "import spacy\n",
        "import bz2\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io # Import the io module\n",
        "\n",
        "# Install required packages in Colab\n",
        "!pip install spacy pandas\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def upload_and_load_data(max_samples=1000):\n",
        "    \"\"\"\n",
        "    Upload and load Amazon review data from bz2 compressed file in Colab\n",
        "    Expected format: __label__1 or __label__2 followed by review text\n",
        "    \"\"\"\n",
        "    print(\"Please upload your .bz2 files (train.ft.txt.bz2 and test.ft.txt.bz2)\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        print(f\"\\nProcessing {filename}...\")\n",
        "        reviews = []\n",
        "        labels = []\n",
        "\n",
        "        try:\n",
        "            # Read the uploaded file\n",
        "            file_content = uploaded[filename]\n",
        "\n",
        "            # Decompress and read\n",
        "            with bz2.open(io.BytesIO(file_content), 'rt', encoding='utf-8') as f:\n",
        "                for i, line in enumerate(f):\n",
        "                    if i >= max_samples:  # Limit for demo purposes\n",
        "                        break\n",
        "\n",
        "                    line = line.strip()\n",
        "                    if line.startswith('__label__'):\n",
        "                        # Extract label and review text\n",
        "                        parts = line.split(' ', 1)\n",
        "                        if len(parts) == 2:\n",
        "                            label = parts[0].replace('__label__', '')\n",
        "                            review_text = parts[1]\n",
        "\n",
        "                            labels.append(int(label))\n",
        "                            reviews.append(review_text)\n",
        "\n",
        "            datasets[filename] = (reviews, labels)\n",
        "            print(f\"Loaded {len(reviews)} reviews from {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "            datasets[filename] = ([], [])\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def load_amazon_data_colab(file_content, max_samples=1000):\n",
        "    \"\"\"\n",
        "    Load Amazon review data from uploaded file content\n",
        "    Expected format: __label__1 or __label__2 followed by review text\n",
        "    \"\"\"\n",
        "    reviews = []\n",
        "    labels = []\n",
        "\n",
        "    try:\n",
        "        # Decompress and read\n",
        "        with bz2.open(io.BytesIO(file_content), 'rt', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= max_samples:  # Limit for demo purposes\n",
        "                    break\n",
        "\n",
        "                line = line.strip()\n",
        "                if line.startswith('__label__'):\n",
        "                    # Extract label and review text\n",
        "                    parts = line.split(' ', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        label = parts[0].replace('__label__', '')\n",
        "                        review_text = parts[1]\n",
        "\n",
        "                        labels.append(int(label))\n",
        "                        reviews.append(review_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return [], []\n",
        "\n",
        "    return reviews, labels\n",
        "\n",
        "class RuleBasedSentimentAnalyzer:\n",
        "    \"\"\"Simple rule-based sentiment analyzer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Positive and negative word lists\n",
        "        self.positive_words = {\n",
        "            'excellent', 'amazing', 'great', 'good', 'fantastic', 'wonderful',\n",
        "            'perfect', 'love', 'awesome', 'brilliant', 'outstanding', 'superb',\n",
        "            'impressed', 'satisfied', 'recommend', 'best', 'quality', 'happy',\n",
        "            'pleased', 'smooth', 'fast', 'easy', 'comfortable', 'durable'\n",
        "        }\n",
        "\n",
        "        self.negative_words = {\n",
        "            'bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'disappointed',\n",
        "            'poor', 'cheap', 'broken', 'defective', 'useless', 'waste', 'annoying',\n",
        "            'frustrating', 'slow', 'difficult', 'uncomfortable', 'flimsy', 'terrible',\n",
        "            'pathetic', 'garbage', 'trash', 'regret', 'problem', 'issue', 'fail'\n",
        "        }\n",
        "\n",
        "        # Negation words that can flip sentiment\n",
        "        self.negation_words = {'not', 'no', 'never', 'none', 'nothing', 'nowhere',\n",
        "                              'neither', 'nobody', 'hardly', \"don't\", \"doesn't\",\n",
        "                              \"didn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\"}\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        \"\"\"Analyze sentiment of text using rule-based approach\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        words = text_lower.split()\n",
        "\n",
        "        positive_score = 0\n",
        "        negative_score = 0\n",
        "\n",
        "        # Check for negation context\n",
        "        negated = False\n",
        "        for i, word in enumerate(words):\n",
        "            # Reset negation after 3 words\n",
        "            if i > 0 and i % 3 == 0:\n",
        "                negated = False\n",
        "\n",
        "            if word in self.negation_words:\n",
        "                negated = True\n",
        "                continue\n",
        "\n",
        "            # Clean word (remove punctuation)\n",
        "            clean_word = re.sub(r'[^\\w]', '', word)\n",
        "\n",
        "            if clean_word in self.positive_words:\n",
        "                if negated:\n",
        "                    negative_score += 1\n",
        "                else:\n",
        "                    positive_score += 1\n",
        "            elif clean_word in self.negative_words:\n",
        "                if negated:\n",
        "                    positive_score += 1\n",
        "                else:\n",
        "                    negative_score += 1\n",
        "\n",
        "        # Determine overall sentiment\n",
        "        if positive_score > negative_score:\n",
        "            return 'positive', positive_score - negative_score\n",
        "        elif negative_score > positive_score:\n",
        "            return 'negative', negative_score - positive_score\n",
        "        else:\n",
        "            return 'neutral', 0\n",
        "\n",
        "def extract_entities_and_sentiment(reviews, labels=None):\n",
        "    \"\"\"Extract named entities and analyze sentiment from reviews\"\"\"\n",
        "    sentiment_analyzer = RuleBasedSentimentAnalyzer()\n",
        "\n",
        "    results = []\n",
        "    entity_counter = Counter()\n",
        "    brand_counter = Counter()\n",
        "    product_counter = Counter()\n",
        "\n",
        "    for i, review in enumerate(reviews):\n",
        "        # Process with spaCy\n",
        "        doc = nlp(review)\n",
        "\n",
        "        # Extract entities\n",
        "        entities = []\n",
        "        brands = []\n",
        "        products = []\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entity_info = {\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'description': spacy.explain(ent.label_)\n",
        "            }\n",
        "            entities.append(entity_info)\n",
        "            entity_counter[f\"{ent.text} ({ent.label_})\"] += 1\n",
        "\n",
        "            # Categorize potential brands and products\n",
        "            if ent.label_ in ['ORG', 'PRODUCT']:\n",
        "                if ent.label_ == 'ORG':\n",
        "                    brands.append(ent.text)\n",
        "                    brand_counter[ent.text] += 1\n",
        "                elif ent.label_ == 'PRODUCT':\n",
        "                    products.append(ent.text)\n",
        "                    product_counter[ent.text] += 1\n",
        "\n",
        "        # Analyze sentiment\n",
        "        sentiment, score = sentiment_analyzer.analyze_sentiment(review)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'review_id': i,\n",
        "            'review_text': review[:200] + '...' if len(review) > 200 else review,\n",
        "            'actual_label': labels[i] if labels else None,\n",
        "            'entities': entities,\n",
        "            'brands': brands,\n",
        "            'products': products,\n",
        "            'sentiment': sentiment,\n",
        "            'sentiment_score': score\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results, entity_counter, brand_counter, product_counter\n",
        "\n",
        "def display_results(results, entity_counter, brand_counter, product_counter, num_samples=5):\n",
        "    \"\"\"Display analysis results\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"AMAZON REVIEWS ANALYSIS RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Sample results\n",
        "    print(f\"\\nSample Analysis Results (showing first {num_samples} reviews):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, result in enumerate(results[:num_samples]):\n",
        "        print(f\"\\nReview {result['review_id'] + 1}:\")\n",
        "        print(f\"Text: {result['review_text']}\")\n",
        "        print(f\"Predicted Sentiment: {result['sentiment']} (score: {result['sentiment_score']})\")\n",
        "        if result['actual_label']:\n",
        "            actual_sentiment = 'positive' if result['actual_label'] == 2 else 'negative'\n",
        "            print(f\"Actual Label: {actual_sentiment}\")\n",
        "\n",
        "        if result['entities']:\n",
        "            print(\"Named Entities:\")\n",
        "            for ent in result['entities']:\n",
        "                print(f\"  - {ent['text']} ({ent['label']}: {ent['description']})\")\n",
        "\n",
        "        if result['brands']:\n",
        "            print(f\"Brands detected: {', '.join(result['brands'])}\")\n",
        "\n",
        "        if result['products']:\n",
        "            print(f\"Products detected: {', '.join(result['products'])}\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\nSUMMARY STATISTICS (Total reviews analyzed: {len(results)})\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Sentiment distribution\n",
        "    sentiment_dist = Counter([r['sentiment'] for r in results])\n",
        "    print(\"\\nSentiment Distribution:\")\n",
        "    for sentiment, count in sentiment_dist.items():\n",
        "        percentage = (count / len(results)) * 100\n",
        "        print(f\"  {sentiment.capitalize()}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    # Top entities\n",
        "    print(f\"\\nTop 10 Named Entities:\")\n",
        "    for entity, count in entity_counter.most_common(10):\n",
        "        print(f\"  {entity}: {count}\")\n",
        "\n",
        "    # Top brands\n",
        "    if brand_counter:\n",
        "        print(f\"\\nTop Brands Detected:\")\n",
        "        for brand, count in brand_counter.most_common(5):\n",
        "            print(f\"  {brand}: {count}\")\n",
        "\n",
        "    # Top products\n",
        "    if product_counter:\n",
        "        print(f\"\\nTop Products Detected:\")\n",
        "        for product, count in product_counter.most_common(5):\n",
        "            print(f\"  {product}: {count}\")\n",
        "\n",
        "def create_summary_dataframe(results):\n",
        "    \"\"\"Create a pandas DataFrame summary of results\"\"\"\n",
        "    data = []\n",
        "    for result in results:\n",
        "        row = {\n",
        "            'review_id': result['review_id'],\n",
        "            'sentiment': result['sentiment'],\n",
        "            'sentiment_score': result['sentiment_score'],\n",
        "            'actual_label': result['actual_label'],\n",
        "            'num_entities': len(result['entities']),\n",
        "            'num_brands': len(result['brands']),\n",
        "            'num_products': len(result['products']),\n",
        "            'review_length': len(result['review_text'])\n",
        "        }\n",
        "        data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def compare_datasets(all_results):\n",
        "    \"\"\"Compare results across different datasets\"\"\"\n",
        "    for filename, data in all_results.items():\n",
        "        results = data['results']\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_reviews = len(results)\n",
        "        sentiment_dist = Counter([r['sentiment'] for r in results])\n",
        "        avg_entities = sum(len(r['entities']) for r in results) / total_reviews\n",
        "\n",
        "        print(f\"\\nüìä {filename}:\")\n",
        "        print(f\"   Total reviews: {total_reviews}\")\n",
        "        print(f\"   Positive: {sentiment_dist.get('positive', 0)} ({sentiment_dist.get('positive', 0)/total_reviews*100:.1f}%)\")\n",
        "        print(f\"   Negative: {sentiment_dist.get('negative', 0)} ({sentiment_dist.get('negative', 0)/total_reviews*100:.1f}%)\")\n",
        "        print(f\"   Neutral: {sentiment_dist.get('neutral', 0)} ({sentiment_dist.get('neutral', 0)/total_reviews*100:.1f}%)\")\n",
        "        print(f\"   Avg entities per review: {avg_entities:.1f}\")\n",
        "\n",
        "# Additional utility functions for further analysis\n",
        "# Moved this function definition before it is called in main_colab\n",
        "def export_results_to_csv(results, filename):\n",
        "    \"\"\"Export results to CSV for further analysis\"\"\"\n",
        "    data = []\n",
        "    for result in results:\n",
        "        row = {\n",
        "            'review_id': result['review_id'],\n",
        "            'review_text': result['review_text'],\n",
        "            'sentiment': result['sentiment'],\n",
        "            'sentiment_score': result['sentiment_score'],\n",
        "            'actual_label': result['actual_label'],\n",
        "            'num_entities': len(result['entities']),\n",
        "            'entities': '; '.join([f\"{e['text']}({e['label']})\" for e in result['entities']]),\n",
        "            'brands': '; '.join(result['brands']),\n",
        "            'products': '; '.join(result['products'])\n",
        "        }\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Results exported to {filename}\")\n",
        "\n",
        "def main_colab():\n",
        "    \"\"\"Main function to run the analysis in Google Colab\"\"\"\n",
        "    print(\"üöÄ Amazon Reviews Analysis with spaCy NER and Rule-based Sentiment\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"üìÅ Upload your .bz2 files when prompted\")\n",
        "\n",
        "    # Upload and load data\n",
        "    datasets = upload_and_load_data(max_samples=500)  # Reduced for Colab performance\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for filename, (reviews, labels) in datasets.items():\n",
        "        if not reviews:\n",
        "            print(f\"‚ö†Ô∏è  No data loaded from {filename}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nüîç Analyzing {filename}...\")\n",
        "        print(f\"üìä Processing {len(reviews)} reviews\")\n",
        "\n",
        "        # Extract entities and sentiment\n",
        "        results, entities, brands, products = extract_entities_and_sentiment(reviews, labels)\n",
        "\n",
        "        # Store results\n",
        "        all_results[filename] = {\n",
        "            'results': results,\n",
        "            'entities': entities,\n",
        "            'brands': brands,\n",
        "            'products': products\n",
        "        }\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nüìà ANALYSIS RESULTS for {filename}\")\n",
        "        print(\"=\"*50)\n",
        "        display_results(results, entities, brands, products)\n",
        "\n",
        "        # Create and display DataFrame\n",
        "        print(f\"\\nüìã Creating summary DataFrame for {filename}...\")\n",
        "        df = create_summary_dataframe(results)\n",
        "        print(df.head())\n",
        "\n",
        "        # Export option\n",
        "        export_filename = filename.replace('.bz2', '_analysis_results.csv')\n",
        "        export_results_to_csv(results, export_filename) # This call will now succeed\n",
        "\n",
        "        print(f\"\\nüíæ Results saved as {export_filename}\")\n",
        "        print(\"üì• You can download it from the Files panel in Colab\")\n",
        "\n",
        "    # Comparison if multiple datasets\n",
        "    if len(all_results) > 1:\n",
        "        print(f\"\\nüîç DATASET COMPARISON\")\n",
        "        print(\"=\"*50)\n",
        "        compare_datasets(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# Quick start function for Colab\n",
        "def quick_start():\n",
        "    \"\"\"Quick start function with smaller sample size for testing\"\"\"\n",
        "    print(\"üöÄ QUICK START - Amazon Reviews Analysis\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"This will process a smaller sample (100 reviews per file) for quick testing\")\n",
        "\n",
        "    datasets = upload_and_load_data(max_samples=100)\n",
        "\n",
        "    for filename, (reviews, labels) in datasets.items():\n",
        "        if reviews:\n",
        "            print(f\"\\nüîç Quick analysis of {filename} ({len(reviews)} reviews):\")\n",
        "            results, entities, brands, products = extract_entities_and_sentiment(reviews, labels)\n",
        "\n",
        "            # Show just summary stats\n",
        "            sentiment_dist = Counter([r['sentiment'] for r in results])\n",
        "            print(f\"   Sentiment: {dict(sentiment_dist)}\")\n",
        "            print(f\"   Top entities: {dict(entities.most_common(3))}\")\n",
        "            print(f\"   Brands found: {list(brands.keys())[:5]}\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For Google Colab - choose your preferred method:\n",
        "\n",
        "    # Option 1: Full analysis (recommended)\n",
        "    results = main_colab()\n",
        "\n",
        "    # Option 2: Quick start for testing (uncomment to use instead)\n",
        "    # results = quick_start()\n",
        "\n",
        "    print(\"\\n‚úÖ Analysis complete! Check the results above and download CSV files from the Files panel.\")\n",
        "\n",
        "# Example usage for CSV export:\n",
        "# export_results_to_csv(train_results, 'train_analysis_results.csv')\n",
        "# export_results_to_csv(test_results, 'test_analysis_results.csv')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YH0eksljiy2u",
        "outputId": "80e1ea8e-badc-4a44-adf8-9e83e139d8ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "üöÄ Amazon Reviews Analysis with spaCy NER and Rule-based Sentiment\n",
            "======================================================================\n",
            "üìÅ Upload your .bz2 files when prompted\n",
            "Please upload your .bz2 files (train.ft.txt.bz2 and test.ft.txt.bz2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6200dc09-d6f8-4e32-9e8d-c50a2487bd0d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6200dc09-d6f8-4e32-9e8d-c50a2487bd0d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.ft.txt.bz2 to test.ft.txt (1).bz2\n",
            "Saving train.ft.txt.bz2 to train.ft.txt (1).bz2\n",
            "\n",
            "Processing test.ft.txt (1).bz2...\n",
            "Loaded 500 reviews from test.ft.txt (1).bz2\n",
            "\n",
            "Processing train.ft.txt (1).bz2...\n",
            "Loaded 500 reviews from train.ft.txt (1).bz2\n",
            "\n",
            "üîç Analyzing test.ft.txt (1).bz2...\n",
            "üìä Processing 500 reviews\n",
            "\n",
            "üìà ANALYSIS RESULTS for test.ft.txt (1).bz2\n",
            "==================================================\n",
            "================================================================================\n",
            "AMAZON REVIEWS ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "Sample Analysis Results (showing first 5 reviews):\n",
            "------------------------------------------------------------\n",
            "\n",
            "Review 1:\n",
            "Text: Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evapor...\n",
            "Predicted Sentiment: positive (score: 3)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Pat (PERSON: People, including fictional)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "  - GREAT (ORG: Companies, agencies, institutions, etc.)\n",
            "  - YEARS (DATE: Absolute or relative dates or periods)\n",
            "  - One (CARDINAL: Numerals that do not fall under another type)\n",
            "  - EVERYBODY (ORG: Companies, agencies, institutions, etc.)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "Brands detected: GREAT, EVERYBODY\n",
            "----------------------------------------\n",
            "\n",
            "Review 2:\n",
            "Text: One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger...\n",
            "Predicted Sentiment: positive (score: 2)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - One (CARDINAL: Numerals that do not fall under another type)\n",
            "  - Chrono Trigger (PERSON: People, including fictional)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "  - one (CARDINAL: Numerals that do not fall under another type)\n",
            "----------------------------------------\n",
            "\n",
            "Review 3:\n",
            "Text: Batteries died within a year ...: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge....\n",
            "Predicted Sentiment: neutral (score: 0)\n",
            "Actual Label: negative\n",
            "Named Entities:\n",
            "  - a year (DATE: Absolute or relative dates or periods)\n",
            "  - 2003 (DATE: Absolute or relative dates or periods)\n",
            "  - about a year (DATE: Absolute or relative dates or periods)\n",
            "----------------------------------------\n",
            "\n",
            "Review 4:\n",
            "Text: works fine, but Maha Energy is better: Check out Maha Energy's website. Their Powerex MH-C204F charger works in 100 minutes for rapid charge, with option for slower charge (better for batteries). And ...\n",
            "Predicted Sentiment: neutral (score: 0)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Maha Energy (PERSON: People, including fictional)\n",
            "  - Their Powerex MH-C204F (ORG: Companies, agencies, institutions, etc.)\n",
            "  - 100 minutes (TIME: Times smaller than a day)\n",
            "  - 2200 (CARDINAL: Numerals that do not fall under another type)\n",
            "Brands detected: Their Powerex MH-C204F\n",
            "----------------------------------------\n",
            "\n",
            "Review 5:\n",
            "Text: Great for the non-audiophile: Reviewed quite a bit of the combo players and was hesitant due to unfavorable reviews and size of machines. I am weaning off my VHS collection, but don't want to replace ...\n",
            "Predicted Sentiment: positive (score: 2)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - VHS (ORG: Companies, agencies, institutions, etc.)\n",
            "Brands detected: VHS\n",
            "----------------------------------------\n",
            "\n",
            "SUMMARY STATISTICS (Total reviews analyzed: 500)\n",
            "==================================================\n",
            "\n",
            "Sentiment Distribution:\n",
            "  Positive: 262 (52.4%)\n",
            "  Neutral: 151 (30.2%)\n",
            "  Negative: 87 (17.4%)\n",
            "\n",
            "Top 10 Named Entities:\n",
            "  first (ORDINAL): 56\n",
            "  one (CARDINAL): 48\n",
            "  two (CARDINAL): 32\n",
            "  Sony (ORG): 22\n",
            "  Amazon (ORG): 18\n",
            "  Christmas (DATE): 17\n",
            "  5 (CARDINAL): 13\n",
            "  2 (CARDINAL): 13\n",
            "  One (CARDINAL): 12\n",
            "  Apple (ORG): 12\n",
            "\n",
            "Top Brands Detected:\n",
            "  Sony: 22\n",
            "  Amazon: 18\n",
            "  Apple: 12\n",
            "  Foundation: 7\n",
            "  eMarker: 6\n",
            "\n",
            "Top Products Detected:\n",
            "  Classic: 2\n",
            "  Leslie: 1\n",
            "  BamPow: 1\n",
            "  W80: 1\n",
            "  Transformers Fan: 1\n",
            "\n",
            "üìã Creating summary DataFrame for test.ft.txt (1).bz2...\n",
            "   review_id sentiment  sentiment_score  actual_label  num_entities  \\\n",
            "0          0  positive                3             2             7   \n",
            "1          1  positive                2             2             5   \n",
            "2          2   neutral                0             1             3   \n",
            "3          3   neutral                0             2             4   \n",
            "4          4  positive                2             2             1   \n",
            "\n",
            "   num_brands  num_products  review_length  \n",
            "0           2             0            203  \n",
            "1           0             0            203  \n",
            "2           0             0            203  \n",
            "3           1             0            203  \n",
            "4           1             0            203  \n",
            "Results exported to test.ft.txt (1)_analysis_results.csv\n",
            "\n",
            "üíæ Results saved as test.ft.txt (1)_analysis_results.csv\n",
            "üì• You can download it from the Files panel in Colab\n",
            "\n",
            "üîç Analyzing train.ft.txt (1).bz2...\n",
            "üìä Processing 500 reviews\n",
            "\n",
            "üìà ANALYSIS RESULTS for train.ft.txt (1).bz2\n",
            "==================================================\n",
            "================================================================================\n",
            "AMAZON REVIEWS ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "Sample Analysis Results (showing first 5 reviews):\n",
            "------------------------------------------------------------\n",
            "\n",
            "Review 1:\n",
            "Text: Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cro...\n",
            "Predicted Sentiment: neutral (score: 0)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Chrono Cross (ORG: Companies, agencies, institutions, etc.)\n",
            "Brands detected: Chrono Cross\n",
            "----------------------------------------\n",
            "\n",
            "Review 2:\n",
            "Text: The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasuno...\n",
            "Predicted Sentiment: positive (score: 2)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Yasunori Mitsuda's (PERSON: People, including fictional)\n",
            "  - years (DATE: Absolute or relative dates or periods)\n",
            "  - every penny (MONEY: Monetary values, including unit)\n",
            "----------------------------------------\n",
            "\n",
            "Review 3:\n",
            "Text: Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promi...\n",
            "Predicted Sentiment: positive (score: 4)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Prisoners of Fate (WORK_OF_ART: Titles of books, songs, etc.)\n",
            "  - A Distant Promise (WORK_OF_ART: Titles of books, songs, etc.)\n",
            "  - Chrono Cross (WORK_OF_ART: Titles of books, songs, etc.)\n",
            "  - Time (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Scar~ (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Chronomantique (PERSON: People, including fictional)\n",
            "  - Chrono Trigger (PERSON: People, including fictional)\n",
            "  - Xenogears (DATE: Absolute or relative dates or periods)\n",
            "  - 6 (CARDINAL: Numerals that do not fall under another type)\n",
            "Brands detected: Time, Scar~\n",
            "----------------------------------------\n",
            "\n",
            "Review 4:\n",
            "Text: Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favo...\n",
            "Predicted Sentiment: positive (score: 3)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Scars Of Time (FAC: Buildings, airports, highways, bridges, etc.)\n",
            "  - Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Drowned Valley (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Two (CARDINAL: Numerals that do not fall under another type)\n",
            "  - Galdorb - Home (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Chronomantique (PERSON: People, including fictional)\n",
            "  - Gale (PERSON: People, including fictional)\n",
            "  - ZelbessDisk (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Three (CARDINAL: Numerals that do not fall under another type)\n",
            "  - three (CARDINAL: Numerals that do not fall under another type)\n",
            "  - Chronopolis (GPE: Countries, cities, states)\n",
            "  - Fates (GPE: Countries, cities, states)\n",
            "  - Jellyfish (NORP: Nationalities or religious or political groups)\n",
            "  - Burning Orphange (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Dragon's Prayer, Tower Of Stars (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Radical Dreamers - Unstealable Jewel (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Xander Cross (PERSON: People, including fictional)\n",
            "Brands detected: Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Drowned Valley, Galdorb - Home, ZelbessDisk, Burning Orphange, Dragon's Prayer, Tower Of Stars, Radical Dreamers - Unstealable Jewel\n",
            "----------------------------------------\n",
            "\n",
            "Review 5:\n",
            "Text: Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are...\n",
            "Predicted Sentiment: positive (score: 3)\n",
            "Actual Label: positive\n",
            "Named Entities:\n",
            "  - Chrono Cross (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Time (ORG: Companies, agencies, institutions, etc.)\n",
            "  - Sea (LOC: Non-GPE locations, mountain ranges, bodies of water)\n",
            "  - Yasunori Mitsuda (PERSON: People, including fictional)\n",
            "Brands detected: Chrono Cross, Time\n",
            "----------------------------------------\n",
            "\n",
            "SUMMARY STATISTICS (Total reviews analyzed: 500)\n",
            "==================================================\n",
            "\n",
            "Sentiment Distribution:\n",
            "  Neutral: 155 (31.0%)\n",
            "  Positive: 249 (49.8%)\n",
            "  Negative: 96 (19.2%)\n",
            "\n",
            "Top 10 Named Entities:\n",
            "  one (CARDINAL): 45\n",
            "  first (ORDINAL): 44\n",
            "  two (CARDINAL): 27\n",
            "  Amazon (ORG): 26\n",
            "  3 (CARDINAL): 17\n",
            "  One (CARDINAL): 16\n",
            "  Thomas (PERSON): 16\n",
            "  5 (CARDINAL): 15\n",
            "  2 (CARDINAL): 15\n",
            "  Barbie (PERSON): 15\n",
            "\n",
            "Top Brands Detected:\n",
            "  Amazon: 26\n",
            "  Cornwell: 9\n",
            "  Scarpetta: 8\n",
            "  HTML: 7\n",
            "  Sony: 6\n",
            "\n",
            "Top Products Detected:\n",
            "  Genealogy: 2\n",
            "  Glitter: 1\n",
            "  Vivendi Games': 1\n",
            "  CLASSIC: 1\n",
            "  Jam: 1\n",
            "\n",
            "üìã Creating summary DataFrame for train.ft.txt (1).bz2...\n",
            "   review_id sentiment  sentiment_score  actual_label  num_entities  \\\n",
            "0          0   neutral                0             2             1   \n",
            "1          1  positive                2             2             3   \n",
            "2          2  positive                4             2             9   \n",
            "3          3  positive                3             2            17   \n",
            "4          4  positive                3             2             4   \n",
            "\n",
            "   num_brands  num_products  review_length  \n",
            "0           1             0            203  \n",
            "1           0             0            203  \n",
            "2           2             0            203  \n",
            "3           7             0            203  \n",
            "4           2             0            203  \n",
            "Results exported to train.ft.txt (1)_analysis_results.csv\n",
            "\n",
            "üíæ Results saved as train.ft.txt (1)_analysis_results.csv\n",
            "üì• You can download it from the Files panel in Colab\n",
            "\n",
            "üîç DATASET COMPARISON\n",
            "==================================================\n",
            "\n",
            "üìä test.ft.txt (1).bz2:\n",
            "   Total reviews: 500\n",
            "   Positive: 262 (52.4%)\n",
            "   Negative: 87 (17.4%)\n",
            "   Neutral: 151 (30.2%)\n",
            "   Avg entities per review: 3.4\n",
            "\n",
            "üìä train.ft.txt (1).bz2:\n",
            "   Total reviews: 500\n",
            "   Positive: 249 (49.8%)\n",
            "   Negative: 96 (19.2%)\n",
            "   Neutral: 155 (31.0%)\n",
            "   Avg entities per review: 3.8\n",
            "\n",
            "‚úÖ Analysis complete! Check the results above and download CSV files from the Files panel.\n"
          ]
        }
      ]
    }
  ]
}